{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 05 [LangChain + Grandes Modelos de Linguagem + API]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 05** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/C1oUi1FKTZ4W9fNdA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **10/12 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:** Eduardo Renesto Estanquiere - 11201810086"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*). Preferencialmente, usar um modelo gratuito. Cada modelo pode ser escolhido por até 4 equipes.\n",
        "\n",
        ">\n",
        "\n",
        "Uma lista de LLMs está disponível em:\n",
        "\n",
        "> https://integrations.langchain.com/llms\n",
        "> https://python.langchain.com/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "_UlblxFxzDV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos da Hugging Face:**\n",
        "\n",
        "* Mistral Base: modelo promissor.\n",
        "\n",
        "> https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "\n",
        "* Mistral Quantizado: mais leve. Pode ser um pouco mais difícil de configurar, mas deve ocupar menos memória.\n",
        "\n",
        "> https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "\n",
        "* Mistral Lite da Amazon: pode ter desempenho melhor.\n",
        "\n",
        "> https://huggingface.co/amazon/MistralLite\n",
        "\n",
        "* Llama 2 7B: modelo da Meta melhorado.\n",
        "\n",
        "> https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "* Llama 2 7b 32k: contexto maior pelo mesmo tamanho.\n",
        "\n",
        "> https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\n",
        "\n",
        "* Galactica: para artigos científicos\n",
        "\n",
        "> https://huggingface.co/facebook/galactica-6.7b\n",
        "\n",
        "* Alpaca: alternativo ao Llama\n",
        "\n",
        "> https://huggingface.co/chavinlo/alpaca-native\n",
        "\n",
        "> https://huggingface.co/spaces/tloen/alpaca-lora"
      ],
      "metadata": {
        "id": "GVpiVhzn3QqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lista de Modelos Interessantes:**\n",
        "\n",
        "> https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html\n",
        "\n",
        "* Vicuna:\n",
        "> https://huggingface.co/lmsys/vicuna-7b-v1.5-16k\n",
        "\n",
        "* Openchat kit:\n",
        "> https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
        "\n",
        "* Meta OPT:\n",
        "> https://huggingface.co/facebook/opt-1.3b\n",
        "\n",
        "* Google Flan:\n",
        "> https://huggingface.co/google/flan-t5-base"
      ],
      "metadata": {
        "id": "WRuAiSr05Ayt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APIs:**\n",
        "\n",
        "* GPT4ALL: tenta ser um chatGPT aberto\n",
        "> https://python.langchain.com/docs/integrations/llms/gpt4all\n",
        "\n",
        "* YandexGPT: da empresa russa que criou um modelo de *Machine Learning* clássico muito bom, o CatBoost.\n",
        "> https://python.langchain.com/docs/integrations/llms/yandex\n",
        "\n",
        "* Anthropic: empresa forte concorrente da OpenAi.\n",
        "> https://python.langchain.com/docs/integrations/chat/anthropic\n",
        "\n",
        "* Gorilla: pipeline para geração de código\n",
        " > https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html"
      ],
      "metadata": {
        "id": "5Q3mhqda5WP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo esse levantamento e comentários foram feitos pelo aluno  **Bruno Sanches Rodrigues**."
      ],
      "metadata": {
        "id": "DO6XMjHx58u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados do LLM selecionada:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**LLM**: `llama-cpp`\n",
        "\n",
        ">\n",
        "\n",
        "**Link para a documentação oficial**: [https://python.langchain.com/docs/integrations/llms/llamacpp](https://python.langchain.com/docs/integrations/llms/llamacpp)\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**Site oficial (GitHub)**: [https://github.com/abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não pode ser o modelo da `OpenAI`."
      ],
      "metadata": {
        "id": "A2oo8GtK0xSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados da API selecionada:\n",
        "\n",
        "`API: ` Last.fm\n",
        "\n",
        "`Site oficial: ` [Last.fm](https://last.fm)\n",
        "\n",
        "`Link para a documentação oficial:` [https://www.last.fm/api](https://www.last.fm/api)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não é necessário usar a mesma **API** da `ATIVIDADE PRÁTICA 03`. Cada **API** pode ser usada por até 4 equipes."
      ],
      "metadata": {
        "id": "bTODq98Myt_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso do framework **`LangChain`** (obrigatório) e de um **LLM** aplicando, no mínimo, uma técnica de PLN. A técnica pode ser aplicada em qualquer córpus. Também é obrigatório usar uma **API** da `ATIVIDADE PRÁTICA 03`. A **API** pode ser usada tanto para obter os dados quanto para disponibilizar os resultados.\n",
        "\n",
        "O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1cOL7zVNffqmliuv23zFm1UJjhEXTdp1Zm5EyAJmhiKg/edit?usp=sharing\n",
        "\n",
        ">\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   Similaridade de Textos\n",
        "*   Reconhecimento de Entidades Nomeadas\n",
        "*   Sistemas de Perguntas e Respostas\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Lista de APIs:**\n",
        "\n",
        "\n",
        "* YouTube\n",
        "* LinkedIn\n",
        "* Twitter (X)\n",
        "* Facebook\n",
        "* Instagram\n",
        "* Medium\n",
        "* Reddit\n",
        "* TikTok\n",
        "* GitHub\n",
        "* Pinterest\n",
        "* Telegram\n",
        "* Dados financeiros\n",
        "* Notícias\n",
        "* Mercado de Ações\n",
        "* Dados financeiros\n",
        "* SMS\n",
        "* OpenAlex\n",
        "* Whisper (OpenAI)\n",
        "* Discord\n",
        "* Slack\n",
        "* Chuck Norris Jokes\n",
        "* Wikipedia\n",
        "* Last.fm\n",
        "* New York Times\n",
        "* Nasdaq Data Link\n",
        "* Yahoo! Finance\n",
        "* Twilio SendGrid Mail Send\n",
        "* Spotify\n",
        "* Awesome API\n",
        "* Google Books API\n",
        "* Mercado Livre API\n",
        "\n",
        ">\n",
        "\n",
        "**PLANILHA DA ATIVIDADE PRÁTICA 03:**\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1-Q1szJ3UmoE2_3LtcRQyqid5fPIcnpsR3XAPnoxLj2o/edit?usp=sharing\n",
        "\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação os segunintes pontos:\n",
        "\n",
        "* Uso do framework **`LangChain`**.\n",
        "\n",
        "* Escolha e uso de um **LLM**.\n",
        "\n",
        "* Escolha e uso de uma **API**\n",
        "\n",
        "* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo o código do notebook deve ser executado. Código sem execução não será considerado."
      ],
      "metadata": {
        "id": "LhwdrMp123Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nosso objetivo é explorar alguns esteriótipos em gêneros de música. Vamos utilizar a API do Last.fm para buscar os principais álbuns de certo gênero. Com essa informação, vamos fazer o `llama-cpp` responder, baseando-se no título do álbum, se ele é `feliz, neutro ou triste`. Em seguida, vamos fazer estatística simples e concluir se os esteriótipos se verificam ou não."
      ],
      "metadata": {
        "id": "0w2TqwlcXXEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Começaremos preparando nossa interação com a API. Esse código foi copiado da Atividade 03 (de minha autoria), com algumas leves alterações."
      ],
      "metadata": {
        "id": "Do8ZH9YNZIsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "BASE_URL      = \"https://ws.audioscrobbler.com/2.0\"\n",
        "API_KEY       = \"44aecac6f877a06ad14045d7b59a8663\"\n",
        "SHARED_SECRET = \"ea38eb51c4229603e2373b734caf8d04\""
      ],
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_request_params(method, args):\n",
        "  default_params = {\n",
        "    \"api_key\": API_KEY,\n",
        "    \"format\": \"json\",\n",
        "    \"method\": method\n",
        "  }\n",
        "\n",
        "  return { **default_params, **args }\n",
        "\n",
        "def tag_get_info(tag):\n",
        "  params = build_request_params(\"tag.getinfo\", {\n",
        "    \"tag\": tag\n",
        "  })\n",
        "\n",
        "  res = requests.get(BASE_URL, params)\n",
        "\n",
        "  return json.loads(res.content)\n",
        "\n",
        "def tag_get_top_albums(tag):\n",
        "  params = build_request_params(\"tag.gettopalbums\", {\n",
        "    \"tag\": tag\n",
        "  })\n",
        "\n",
        "  res = requests.get(BASE_URL, params)\n",
        "\n",
        "  j = json.loads(res.content)\n",
        "\n",
        "  return [ album[\"name\"] for album in j[\"albums\"][\"album\"] ]"
      ],
      "metadata": {
        "id": "IaX9Zd8Su4Tp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos começar o preparo do modelo.\n",
        "\n",
        "Utilizaremos o `llama-cpp`, que precisa de um arquivo externo para ser carregado. Utilizaremos o `llama-2-13b-chat` no nível de quantização Q4, que nos permite executar dentro das limitações do Colab.\n",
        "\n",
        "**OBSERVAÇÃO**: Usaremos aceleração da GPU dentro do Colab. Portanto, é necessário escolher o tipo *T4* de instância."
      ],
      "metadata": {
        "id": "e1uFdcHhZecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98YgFvZLu5SN",
        "outputId": "8404452b-4249-412a-bef7-b743c3b2373b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-12 22:49:51--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.114, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/7ddfe27f61bf994542c22aca213c46ecbd8a624cca74abff02a7b5a8c18f787f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q4_K_M.gguf%22%3B&Expires=1702677001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY3NzAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzdkZGZlMjdmNjFiZjk5NDU0MmMyMmFjYTIxM2M0NmVjYmQ4YTYyNGNjYTc0YWJmZjAyYTdiNWE4YzE4Zjc4N2Y%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=snVXogDfciDrG4XYbPEFTZ92BrjuxcNblrnSqVXxvGKQsd6hwMOiir2FXgcj3Ai4v35z%7ExGB--EwiRyz-K7elCL--neIE5S7EDxRo5b8Q%7EJiuDea9zRJmQqKly%7EoVdaFcmsFyHKL6dcBbpxLi7Pk8MW1WxDoQYLvaaO7UEG59HDIcID8mg%7ELc0%7Ei%7EXIfSiXzsTgdI6C56BhEuqS5FnZRiUUjd8Som5EleU62XUyO%7EncIdxsQM8SlZEtZb7mFDGah2Udf0TQ9DLZD41hIdUJEeRXgdmvSMs0EBv%7EaXgmHGXG8LhpLAiM66r2ucBVnG1kJNsbv-6MaaawqjNKZnwYx4w__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-12 22:49:52--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/7ddfe27f61bf994542c22aca213c46ecbd8a624cca74abff02a7b5a8c18f787f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q4_K_M.gguf%22%3B&Expires=1702677001&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY3NzAwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwLzdkZGZlMjdmNjFiZjk5NDU0MmMyMmFjYTIxM2M0NmVjYmQ4YTYyNGNjYTc0YWJmZjAyYTdiNWE4YzE4Zjc4N2Y%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=snVXogDfciDrG4XYbPEFTZ92BrjuxcNblrnSqVXxvGKQsd6hwMOiir2FXgcj3Ai4v35z%7ExGB--EwiRyz-K7elCL--neIE5S7EDxRo5b8Q%7EJiuDea9zRJmQqKly%7EoVdaFcmsFyHKL6dcBbpxLi7Pk8MW1WxDoQYLvaaO7UEG59HDIcID8mg%7ELc0%7Ei%7EXIfSiXzsTgdI6C56BhEuqS5FnZRiUUjd8Som5EleU62XUyO%7EncIdxsQM8SlZEtZb7mFDGah2Udf0TQ9DLZD41hIdUJEeRXgdmvSMs0EBv%7EaXgmHGXG8LhpLAiM66r2ucBVnG1kJNsbv-6MaaawqjNKZnwYx4w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.25, 108.138.94.23, 108.138.94.122, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7865956224 (7.3G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q4_K_M.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q4 100%[===================>]   7.33G   264MB/s    in 39s     \n",
            "\n",
            "2023-12-12 22:50:31 (192 MB/s) - ‘llama-2-13b-chat.Q4_K_M.gguf’ saved [7865956224/7865956224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kljtZ933BtGl",
        "outputId": "a2d8f1ed-e06d-4f10-bf81-c5c4efa8a3d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.22)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfrR1KzrCMPt",
        "outputId": "d6eadc41-2bdd-487b-a7eb-1655155a056e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.350)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.2)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.0)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "psi51gE6FX1H"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, começam as funções necessárias para construir a pipeline do LangChain."
      ],
      "metadata": {
        "id": "USldIKpYZ8eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_llm():\n",
        "  llm = LlamaCpp(\n",
        "      model_path=\"./llama-2-13b-chat.Q4_K_M.gguf\",\n",
        "      temperature=0.75,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      n_gpu_layers=40,\n",
        "      n_batch=128\n",
        "  )\n",
        "\n",
        "  return llm\n",
        "\n",
        "def build_chain(llm):\n",
        "  #template = \"\"\"Classify the title 'A Crow Looked At Me' as sad, neutral, or happy: sad\n",
        "  #Classify the title 'Lover' as sad, neutral or happy: happy\n",
        "  #Classify the title '{title}' as sad, neutral or happy:\n",
        "  #\"\"\"\n",
        "\n",
        "  template = \"\"\"[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest music reviewer assistant. You'll receive an album name, and you should answer whether it is happy, sad, or neutral. Your answer should be succint and contain only a single word.\n",
        "<</SYS>>\n",
        "{title}[/INST]\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"title\"])\n",
        "\n",
        "  chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "  return chain"
      ],
      "metadata": {
        "id": "bJgjJ_XDEd-r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com isso, podemos finalmente construir o modelo e carregá-lo na memória:"
      ],
      "metadata": {
        "id": "aUKhctKGaGxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = build_llm()\n",
        "chain = build_chain(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAhIwGqfGmwp",
        "outputId": "773803c3-0e54-4e1d-a0e4-8f93ab4cda91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar algumas funções para utilidade. `classify_album` irá receber um título de álbum, e perguntar ao LLaMa qual sentimento ele passa. Note que o modelo nem sempre responde dentro das imposições do prompt: às vezes, a resposta é bem grande e sem nexo. Portanto, como maneira de limpar essas divergências, ignoramos todas as respostas que tenham mais de uma palavra."
      ],
      "metadata": {
        "id": "s4tyqTGEaOh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_album(chain, title):\n",
        "  answer = chain.run(title=title)\n",
        "  answer_stripped = answer.strip()\n",
        "\n",
        "  if len(answer_stripped.split()) != 1:\n",
        "    return None\n",
        "\n",
        "  return answer_stripped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZQdQxdhOmli",
        "outputId": "94874d1a-f3e2-45f0-db16-f66f07d920b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assim, podemos construir `classify_tag`, que bate na API do Last.fm, pega os principais discos de um gênero, e chama a função `classify_album` para cada um dos resultados. Ao final, calcula a distribuição das emoções encontrada."
      ],
      "metadata": {
        "id": "NY3XASi8am3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def classify_tag(chain, tag):\n",
        "  albums = tag_get_top_albums(tag)\n",
        "  classifications = list(filter(lambda x: x != None, [ classify_album(chain, album) for album in albums ]))\n",
        "  total = len(classifications)\n",
        "\n",
        "  c = Counter(classifications)\n",
        "\n",
        "  freq_happy = c[\"Happy\"] / total\n",
        "  freq_neutral = c[\"Neutral\"] / total\n",
        "  freq_sad = c[\"Sad\"] / total\n",
        "\n",
        "  return { \"happy\": freq_happy, \"neutral\": freq_neutral, \"sad\": freq_sad }"
      ],
      "metadata": {
        "id": "vELJ2r-SRGJ8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, definimos uma lista de gêneros a serem investigados, chamamos `classify_tag` em cada um deles, e fazemos uma breve análise, encontrando o gênero mais triste e o gênero mais feliz.\n",
        "\n",
        "A lista está parcialmente comentada por razões de tempo de processamento. Caso deseje testar, basta descomentar os gêneros comentados e adicionar outros, caso queira.\n",
        "\n",
        "**NOTA:** As saídas que dizem `Llama.generate: ...` são logs internos do modelo e infelizmente não há como desligá-los. Favor, desconsidere."
      ],
      "metadata": {
        "id": "aWtFcJK1a7hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = [\n",
        "    \"shoegaze\",\n",
        "    \"black metal\",\n",
        "    \"synthpop\",\n",
        "    \"samba\",\n",
        "    #\"hyperpop\",\n",
        "    #\"indie rock\",\n",
        "    #\"art rock\",\n",
        "    #\"djent\",\n",
        "    #\"jazz\"\n",
        "]\n",
        "\n",
        "indices = [ [genre, classify_tag(chain, genre)] for genre in genres ]\n",
        "\n",
        "for [genre, classification] in indices:\n",
        "  happy = classification[\"happy\"] * 100\n",
        "  neutral = classification[\"neutral\"] * 100\n",
        "  sad = classification[\"neutral\"] * 100\n",
        "\n",
        "  print(f\"- {genre} é {happy}% feliz, {neutral}% neutro e {sad}% triste\")\n",
        "\n",
        "happiest = max(indices, key=lambda x: x[1][\"happy\"])\n",
        "saddest  = max(indices, key=lambda x: x[1][\"sad\"])\n",
        "\n",
        "print(f\"O gênero mais feliz é {happiest[0]}\")\n",
        "print(f\"O gênero mais triste é {saddest[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Y3tftvVEbL",
        "outputId": "3e5fd7ca-025b-4814-a0c5-2fda8282f8df"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- shoegaze é 19.565217391304348% feliz, 67.3913043478261% neutro e 67.3913043478261% triste\n",
            "- black metal é 6.382978723404255% feliz, 65.95744680851064% neutro e 65.95744680851064% triste\n",
            "- synthpop é 50.0% feliz, 32.608695652173914% neutro e 32.608695652173914% triste\n",
            "- samba é 47.82608695652174% feliz, 23.91304347826087% neutro e 23.91304347826087% triste\n",
            "O gênero mais feliz é synthpop\n",
            "O gênero mais triste é black metal\n"
          ]
        }
      ]
    }
  ]
}